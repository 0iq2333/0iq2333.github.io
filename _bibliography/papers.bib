---
---

@inproceedings{FedMix,
  abbr={ICNC},
  author = {Zhiyao Pan and
            Zhongyi Chang and
            Ting Li and
            Shaofu Yang},
  title = {Distributed Optimization via Mixing ADAM and Quasi-Newton Methods},
  booktitle = {ICNC},
  year = {2024},
  abstract={The heterogeneous computational capacities of devices always negatively impact the performance of distributed optimization algorithms. In this paper, we propose a novel distributed optimization algorithm based on a mixture of different local optimizers, termed as FedMix, which has the potential to leverage the heterogeneity to enhance the performance. Specifically, FedMix is designed in the parameter-server framework, and each client can be equipped with either an Adam optimizer or a quasi-Newton optimizer for local updates based on its computational capability, which leads to a mixture of Adam and Quasi-Newton in our algorithm. As different local optimizers demand different computational capabilities, our algorithm FedMix can achieve better alignment between algorithms and computing resources, facilitating improved performance especially in the heterogeneous computational environment. Sufficient experiments are presented to substantiate the superior performance of our algorithm by comparing with existing algorithms.},
  bibtex_show={true},
  selected={true},
}


@inproceedings{FedMix,
  abbr={ICICIP},
  author = {Ting Li Pan and
            Zhongyi Chang and
            Zhiyao Pan and
            Shaofu Yang and
            Wenying Xu and
            Zhongying Chen},
  title = {Decentralized Federated Learning with Local Auto-switchable Optimizers},
  booktitle = {ICICIP},
  year = {2024},
  abstract={In this paper, we propose a novel decentralized optimization algorithm DLAGD for non-convex problems, in which each node is equipped with an auto-switchable local optimizer. The local optimizer is a combination of gradient descent with an adaptive stepsize scheme. In particular, the stepsize scheme allows the optimizer switch between the SGD with momentum and an Adam-like algorithm, then is more flexible than existing distributed algorithms using only one type of local optimizer. Consequently, DLAGD can better adapt to the structure of the objective function and has the potential to accelerate the convergence rate. For non-convex problems, we theoretically prove that DLAGD can converge to a stationary point. Finally, numerical experiments are presented to show the superior performance of DLAGD.},
  bibtex_show={true},
  selected={true},
}