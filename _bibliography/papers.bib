---
---
@string{aps = {American Physical Society,}}

@Article{FF-Chang-Zhang2024,
  author      = {Zhongyi Chang and Zhen Zhang and Shaofu Yang and Jinde Cao},
  journal     = {Fractal and Fractional},
  title       = {A Flexible Framework for Decentralized Composite Optimization with Compressed Communication},
  year        = {2024},
  abbr        = {FF},
  abstract    = {This paper addresses the decentralized composite convex optimization problem, where agents in a network collaborate through communication and local computation to minimize the sum of their local objective functions and non-smooth terms. We propose a communication-efficient algorithm framework, termed communication-efficient decentralized ADMM (CE-DADMM), by combining the ADMM framework with the three-point compressed (3PC) communication mechanism. This framework not only covers existing mainstream communication-efficient algorithms but also introduces a series of new algorithms. One of the key features of the CE-DADMM framework is its flexibility, allowing it to adapt to different communication and computation needs, balancing communication efficiency and computational overhead. Notably, when employing quasi-Newton updates, CE-DADMM becomes the first communication-efficient second-order algorithm based on compression that can efficiently handle composite optimization problems. Theoretical analysis shows that, even in the presence of compression errors, the proposed algorithm maintains exact linear convergence when the local objective functions are strongly convex. Finally, numerical experiments demonstrate the algorithm's impressive communication efficiency.},
  bibtex_show = {true},
  selected    = {true},
}
  % number      = {5},
  % pages       = {6679-6692},
  % volume      = {35},
    html        = {https://ieeexplore.ieee.org/abstract/document/9923641},
  pdf         = {TNNLS-Li-Wang2023.pdf},
@InProceedings{ICNC-Pan-Chang-Li2024,
  author  = {Zhiyao Pan and
            Zhongyi Chang and
            Ting Li and
            Shaofu Yang},
  booktitle    = {Proc. of 3th International Conference on Neuromorphic Computing},
  title        = {Decentralized Federated Learning with Local Auto-switchable Optimizers},
  year         = {2024},
  organization = {IEEE},
  abbr         = {ICNC},
  abstract={The heterogeneous computational capacities of devices always negatively impact the performance of distributed optimization algorithms. In this paper, we propose a novel distributed optimization algorithm based on a mixture of different local optimizers, termed as FedMix, which has the potential to leverage the heterogeneity to enhance the performance. Specifically, FedMix is designed in the parameter-server framework, and each client can be equipped with either an Adam optimizer or a quasi-Newton optimizer for local updates based on its computational capability, which leads to a mixture of Adam and Quasi-Newton in our algorithm. As different local optimizers demand different computational capabilities, our algorithm FedMix can achieve better alignment between algorithms and computing resources, facilitating improved performance especially in the heterogeneous computational environment. Sufficient experiments are presented to substantiate the superior performance of our algorithm by comparing with existing algorithms.},
  bibtex_show  = {true},
  selected={true},
}

@InProceedings{ICICIP-Li-Chang-Pan2024,
  author       = {Ting Li Pan and
            Zhongyi Chang and
            Zhiyao Pan and
            Shaofu Yang and
            Wenying Xu and
            Zhongying Chen},
  booktitle    = {Proc. of 13th International Conference on Intelligent Control and Information Processing},
  title        = {Decentralized Federated Learning with Local Auto-switchable Optimizers},
  year         = {2024},
  organization = {IEEE},
  abbr         = {ICICIP},
  abstract={In this paper, we propose a novel decentralized optimization algorithm DLAGD for non-convex problems, in which each node is equipped with an auto-switchable local optimizer. The local optimizer is a combination of gradient descent with an adaptive stepsize scheme. In particular, the stepsize scheme allows the optimizer switch between the SGD with momentum and an Adam-like algorithm, then is more flexible than existing distributed algorithms using only one type of local optimizer. Consequently, DLAGD can better adapt to the structure of the objective function and has the potential to accelerate the convergence rate. For non-convex problems, we theoretically prove that DLAGD can converge to a stationary point. Finally, numerical experiments are presented to show the superior performance of DLAGD.},
  bibtex_show  = {true},
  selected={true},
}
  % pages        = {352--359},
  % pdf          = {Index_Tracking_Based_on_Dynamic_Time_Warping_and_Constrained_k-medoids_Clustering.pdf},


